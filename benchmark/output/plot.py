# GENERATED BY chatGPT (o1) on Jan 21, 2025
import os
import json
import pandas as pd
import plotly.graph_objects as go
from plotly.offline import plot

###############################################################################
# 1. LOAD NODE METRICS
###############################################################################

def load_metrics_from_output_folders(base_dir='.'):
    """
    Recursively search for 'output-<node_id>' folders under base_dir.
    For each folder, read 'node-<node_id>.json' and extract 'metrics'.
    
    Returns a DataFrame with columns like:
       ['node_id', 'timestamp', 'redundancy', 'peers_count', ...]
    """
    records = []
    
    for item in os.listdir(base_dir):
        folder_path = os.path.join(base_dir, item)
        if os.path.isdir(folder_path) and item.startswith("output-"):
            # extract node_id
            _, node_id_str = item.split("-", maxsplit=1)
            
            node_json = os.path.join(folder_path, f"node-{node_id_str}.json")
            if os.path.isfile(node_json):
                with open(node_json, 'r') as f:
                    data = json.load(f)
                for entry in data.get("metrics", []):
                    entry["node_id"] = node_id_str
                    records.append(entry)
    
    if not records:
        return pd.DataFrame()
    return pd.DataFrame(records)

def create_metrics_figure(df, default_metric="redundancy"):
    """
    Create an interactive line chart of node metrics over time, in ms since first event.
    Returns a Plotly Figure object (or None if no data).
    """
    if df.empty:
        return None
    
    # Compute "relative_timestamp" = ms offset from earliest event
    t0 = df["timestamp"].min()
    df["relative_timestamp"] = df["timestamp"] - t0
    
    # Determine which columns are metrics
    ignore_cols = {"timestamp", "node_id", "relative_timestamp"}
    metric_cols = [c for c in df.columns if c not in ignore_cols]
    if not metric_cols:
        return None
    
    if default_metric not in metric_cols:
        default_metric = metric_cols[0]
    
    metric_cols.sort()
    node_ids = df["node_id"].unique()
    
    fig = go.Figure()
    metric_to_trace_indices = {m: [] for m in metric_cols}
    
    # Add one trace per (metric, node)
    for metric in metric_cols:
        for node in node_ids:
            node_df = df[df["node_id"] == node].sort_values("relative_timestamp")
            trace = go.Scatter(
                x=node_df["relative_timestamp"],
                y=node_df[metric],
                mode='lines+markers',
                name=f"Node {node}",
                visible=(metric == default_metric)
            )
            fig.add_trace(trace)
            metric_to_trace_indices[metric].append(len(fig.data) - 1)
    
    # Build dropdown to switch metrics
    def make_visibility_array(selected_metric):
        total = len(fig.data)
        visible = [False] * total
        for idx in metric_to_trace_indices[selected_metric]:
            visible[idx] = True
        return visible
    
    buttons = []
    for m in metric_cols:
        buttons.append(
            dict(
                label=m,
                method='update',
                args=[
                    {"visible": make_visibility_array(m)},
                    {"title": f"Metric: {m}"}
                ]
            )
        )
    
    updatemenus = [
        dict(
            active=metric_cols.index(default_metric),
            x=0.0,
            xanchor="left",
            y=1.15,
            yanchor="top",
            buttons=buttons
        )
    ]
    
    max_rel_ts = df["relative_timestamp"].max()
    
    fig.update_layout(
        title=f"Metric: {default_metric}",
        xaxis=dict(
            title="Milliseconds since first event",
            range=[0, max_rel_ts]
        ),
        yaxis=dict(title="Value (unitless)"),
        updatemenus=updatemenus,
        legend=dict(
            orientation="v",
            x=1.02,
            xanchor="left",
            y=1.0,
            yanchor="top"
        ),
        margin=dict(l=50, r=200, t=100, b=50)
    )
    return fig

###############################################################################
# 2. LOAD PUBLISHED & DELIVERED DATA
###############################################################################

def load_published_and_delivered(base_dir='.'):
    """
    Scans 'output-<node_id>' directories for:
      - published_node-<node_id>.json
      - delivered_node-<node_id>.json
    
    Returns three items:
      published_by_node: dict[node_id -> dict[tx_id -> publish_time_ms]]
      earliest_publish_times: dict[tx_id -> earliest_publish_time_ms]
      latest_delivery_times:  dict[tx_id -> latest_delivery_time_ms]
    """
    published_by_node = {}
    earliest_publish_times = {}
    latest_delivery_times = {}
    
    for item in os.listdir(base_dir):
        folder_path = os.path.join(base_dir, item)
        if os.path.isdir(folder_path) and item.startswith("output-"):
            # node_id
            _, node_id_str = item.split("-", maxsplit=1)
            
            if node_id_str not in published_by_node:
                published_by_node[node_id_str] = {}
            
            # published file
            published_json = os.path.join(folder_path, f"published_node-{node_id_str}.json")
            if os.path.isfile(published_json):
                with open(published_json, 'r') as f:
                    data = json.load(f)
                for (tx_id, pub_time) in data.get("published", []):
                    # store in per-node dict
                    published_by_node[node_id_str][tx_id] = pub_time
                    # track earliest publish globally
                    if tx_id not in earliest_publish_times:
                        earliest_publish_times[tx_id] = pub_time
                    else:
                        if pub_time < earliest_publish_times[tx_id]:
                            earliest_publish_times[tx_id] = pub_time
            
            # delivered file
            delivered_json = os.path.join(folder_path, f"delivered_node-{node_id_str}.json")
            if os.path.isfile(delivered_json):
                with open(delivered_json, 'r') as f:
                    data = json.load(f)
                for (tx_id, del_time) in data.get("delivered", []):
                    # track latest delivery globally
                    if tx_id not in latest_delivery_times:
                        latest_delivery_times[tx_id] = del_time
                    else:
                        if del_time > latest_delivery_times[tx_id]:
                            latest_delivery_times[tx_id] = del_time
    
    return published_by_node, earliest_publish_times, latest_delivery_times

###############################################################################
# 3. COMPUTE DISTRIBUTION TIMES
###############################################################################

def compute_global_worstcase_distribution_times(earliest_publish_times, latest_delivery_times):
    """
    For each transaction, worst-case distribution = (latest_delivery - earliest_publish).
    Returns a list of distribution times in ms.
    """
    dist_times = []
    for tx_id, pub_time in earliest_publish_times.items():
        if tx_id in latest_delivery_times:
            latest_del = latest_delivery_times[tx_id]
            dist_ms = latest_del - pub_time
            dist_times.append(dist_ms)
    return dist_times

def compute_per_node_published_distribution_times(published_by_node, latest_delivery_times):
    """
    Build a dict[node_id -> list of distribution times (ms)].
    
    For each node, for each tx that *this node published*, we compute:
        distribution_time = (latest_delivery_times[tx] - this_node_publish_time)
    if that tx actually appears in latest_delivery_times.
    """
    dist_times_per_node = {}
    
    for node_id, tx_map in published_by_node.items():
        # tx_map: dict[tx_id -> publish_time_ms for this node]
        dist_list = []
        for tx_id, pub_time in tx_map.items():
            if tx_id in latest_delivery_times:
                latest_del = latest_delivery_times[tx_id]
                dist_ms = latest_del - pub_time
                dist_list.append(dist_ms)
        dist_times_per_node[node_id] = dist_list
    return dist_times_per_node

###############################################################################
# 4. CREATE VIOLIN PLOTS
###############################################################################

def create_distribution_violin_global(distribution_times):
    """
    A single violin with all global worst-case distribution times (in ms).
    """
    if not distribution_times:
        return None
    
    fig = go.Figure()
    fig.add_trace(go.Violin(
        y=distribution_times,
        meanline_visible=True,
        name='Worst-case Dist Times (ms)'
    ))
    
    fig.update_layout(
        title="Global Worst-case Distribution Times",
        yaxis=dict(title="Time (milliseconds)")
    )
    return fig

def create_distribution_violin_per_node(dist_times_per_node):
    """
    One violin per node, each showing distribution times for the txs
    published by that node.
    """
    if not dist_times_per_node:
        return None
    
    fig = go.Figure()
    # We'll sort node IDs to have a consistent order on the x-axis
    sorted_nodes = sorted(dist_times_per_node.keys(), key=lambda x: int(x) if x.isdigit() else x)
    
    for node_id in sorted_nodes:
        times = dist_times_per_node[node_id]
        if times:  # only add a trace if there's data
            fig.add_trace(go.Violin(
                y=times,
                x=[f"Node {node_id}"] * len(times),
                meanline_visible=True,
                name=f"Node {node_id}"
            ))
    
    fig.update_layout(
        title="Per-node Published Distribution Times",
        yaxis=dict(title="Time (milliseconds)"),
        xaxis=dict(title="Node ID", type="category"),
        violinmode="group"
    )
    return fig

###############################################################################
# 5. WRITE .DAT FILES
###############################################################################

def write_per_node_metric_dat_files(df_metrics, prefix_dir, prefix):
    """
    For each node *and* for each metric, create a .dat file:
      prefix/node-<node_id>/<prefix>_node_<node_id>_<metric>.dat
    Lines of the form:
      <relative_timestamp> <metric_value>
    sorted by ascending relative_timestamp.
    
    Example:
      "myrun/node-1/myrun_node_1_redundancy.dat"
         0 1.23
         500 1.50
         ...
    """
    if df_metrics.empty:
        return
    
    # Compute relative_timestamp
    t0 = df_metrics["timestamp"].min()
    df_metrics["relative_timestamp"] = df_metrics["timestamp"] - t0
    
    ignore_cols = {"timestamp", "node_id", "relative_timestamp"}
    metric_cols = [c for c in df_metrics.columns if c not in ignore_cols]
    metric_cols.sort()
    
    # We'll group by node, then for each metric we'll write a file
    grouped = df_metrics.groupby("node_id")
    
    for node_id, group_df in grouped:
        # Make subfolder for this node
        node_folder = os.path.join(prefix_dir, f"node-{node_id}")
        os.makedirs(node_folder, exist_ok=True)
        
        # Sort by relative_timestamp
        group_df = group_df.sort_values("relative_timestamp")
        
        # For each metric, write a separate file
        for metric in metric_cols:
            out_path = os.path.join(node_folder, f"{prefix}_node_{node_id}_{metric}.dat")
            with open(out_path, "w") as f:
                for _, row in group_df.iterrows():
                    rel_ts = row["relative_timestamp"]
                    val = row[metric]
                    f.write(f"{rel_ts} {val}\n")

def write_global_distribution_dat_file(global_times, prefix_dir, prefix):
    """
    Write <prefix>_global_distribution.dat in prefix_dir, lines of the form:
      <dist_time>
    """
    if not global_times:
        return
    out_path = os.path.join(prefix_dir, f"{prefix}_global_distribution.dat")
    with open(out_path, "w") as f:
        for dist in global_times:
            f.write(f"{dist}\n")

def write_node_distribution_dat_files(dist_times_per_node, prefix_dir, prefix):
    """
    For each node, create prefix/node-<node_id>/<prefix>_node_<node_id>_distribution.dat
    containing lines:
       <dist_time>
    """
    for node_id, dist_list in dist_times_per_node.items():
        if dist_list:
            node_folder = os.path.join(prefix_dir, f"node-{node_id}")
            os.makedirs(node_folder, exist_ok=True)
            
            out_path = os.path.join(node_folder, f"{prefix}_node_{node_id}_distribution.dat")
            with open(out_path, "w") as f:
                for dist in dist_list:
                    f.write(f"{dist}\n")

###############################################################################
# 6. MAIN
###############################################################################

def main():
    # Ask the user for a prefix
    prefix = input("Enter the prefix for the data files (and plots): ").strip()
    if not prefix:
        print("No prefix given. Exiting.")
        return
    
    # Create top-level prefix folder
    os.makedirs(prefix, exist_ok=True)
    
    # A) LOAD METRICS
    df_metrics = load_metrics_from_output_folders(base_dir='.')
    
    # Build the interactive metrics figure
    metrics_fig = create_metrics_figure(df_metrics, default_metric="redundancy")
    
    # B) LOAD PUBLISHED/DELIVERED & DISTRIBUTION
    published_by_node, earliest_publish_times, latest_delivery_times = load_published_and_delivered(base_dir='.')
    global_times = compute_global_worstcase_distribution_times(earliest_publish_times, latest_delivery_times)
    dist_times_per_node = compute_per_node_published_distribution_times(published_by_node, latest_delivery_times)
    
    # B1) Build distribution figures
    global_fig = create_distribution_violin_global(global_times)
    node_fig  = create_distribution_violin_per_node(dist_times_per_node)
    
    #---------------------------------------------------------------------------
    # WRITE .DAT FILES
    #---------------------------------------------------------------------------
    
    # 1. Metrics data: one .dat file per node per metric
    if not df_metrics.empty:
        write_per_node_metric_dat_files(df_metrics, prefix, prefix)
    else:
        print("No node metrics found.")
    
    # 2. Global distribution
    if global_times:
        write_global_distribution_dat_file(global_times, prefix, prefix)
    else:
        print("No global distribution data found.")
    
    # 3. Per-node distribution
    if dist_times_per_node:
        write_node_distribution_dat_files(dist_times_per_node, prefix, prefix)
    else:
        print("No per-node distribution data found.")
    
    #---------------------------------------------------------------------------
    # CREATE & SAVE HTML PLOTS
    #---------------------------------------------------------------------------
    
    if metrics_fig is not None:
        html_path = os.path.join(prefix, f"{prefix}_metrics_plot.html")
        plot(metrics_fig, filename=html_path, auto_open=True)
    else:
        print("No node metrics to plot.")
    
    if global_fig is not None:
        html_path = os.path.join(prefix, f"{prefix}_distribution_violin_global.html")
        plot(global_fig, filename=html_path, auto_open=True)
    else:
        print("No global distribution to plot.")
    
    if node_fig is not None:
        html_path = os.path.join(prefix, f"{prefix}_distribution_violin_per_node.html")
        plot(node_fig, filename=html_path, auto_open=True)
    else:
        print("No per-node distribution to plot.")

if __name__ == "__main__":
    main()
